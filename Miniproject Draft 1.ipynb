{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e2a86dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Sequential\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import shutil\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2f5c0770",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset_dir = '\\\\Users\\\\Sadie Nathan\\\\Documents\\\\GitHub\\\\Term 2\\\\Mini Project\\\\compounded_data' #\"C:\\Users\\Sadie Nathan\\Documents\\GitHub\\Term 2\\Mini Project\\compounded_data\"\n",
    "#Output_dir = 'output'\n",
    "#try:\n",
    "#        os.makedir(Dataset_dir)\n",
    "#        os.makedir(Output_dir)\n",
    "#except:\n",
    "#    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "69083c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'compounded_data' created successfully\n"
     ]
    }
   ],
   "source": [
    "directory = \"compounded_data\"\n",
    " \n",
    "# Parent Directory path\n",
    "parent_dir = \"\\\\Users\\\\Sadie Nathan\\\\Documents\\\\GitHub\\\\Term 2\\\\Mini Project\\\\\"\n",
    " \n",
    "# Path\n",
    "path = os.path.join(parent_dir, directory)\n",
    " \n",
    "\n",
    "try:\n",
    "    os.makedirs(path, exist_ok = True)\n",
    "    print(\"Directory '%s' created successfully\" %directory)\n",
    "except OSError as error:\n",
    "    print(\"Directory '%s' can not be created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0e98a27b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data (1).jpg',\n",
       " 'data (10).jpg',\n",
       " 'data (100).jpg',\n",
       " 'data (1000).jpg',\n",
       " 'data (10000).jpg']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_list = os.listdir(path) \n",
    "dir_list[:5] #for some reason it isn't able to show the full number of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d8840177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: split-folders in c:\\users\\sadie nathan\\appdata\\roaming\\python\\python39\\site-packages (0.5.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install split-folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9b8c50ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'split_folders'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [53]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msplit_folders\u001b[39;00m \u001b[38;5;66;03m# or import splitfolders\u001b[39;00m\n\u001b[0;32m      2\u001b[0m input_folder \u001b[38;5;241m=\u001b[39m path\n\u001b[0;32m      3\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mSadie Nathan\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDocuments\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mGitHub\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mTerm 2\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mMini Project\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mOutput\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m#where you want the split datasets saved. one will be created if it does not exist or none is set\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'split_folders'"
     ]
    }
   ],
   "source": [
    "import split_folders # or import splitfolders\n",
    "input_folder = path\n",
    "output = \"\\\\Users\\\\Sadie Nathan\\\\Documents\\\\GitHub\\\\Term 2\\\\Mini Project\\\\Output\" #where you want the split datasets saved. one will be created if it does not exist or none is set\n",
    "\n",
    "split_folders.ratio(input_folder, output=output, seed=42, ratio=(.8, .1, .1)) # ratio of split are in order of train/val/test. You can change to whatever you want. For train/val sets only, you could do .75, .25 for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c3d435d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --data_path DATA_PATH --test_data_path_to_save TEST_DATA_PATH_TO_SAVE --train_ratio\n",
      "                             TRAIN_RATIO\n",
      "ipykernel_launcher.py: error: the following arguments are required: --data_path, --test_data_path_to_save, --train_ratio\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3377: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "def get_files_from_folder(path):\n",
    "\n",
    "    files = os.listdir(path)\n",
    "    return np.asarray(files)\n",
    "\n",
    "def main(path_to_data, path_to_test_data, train_ratio):\n",
    "    # get dirs\n",
    "    _, dirs, _ = next(os.walk(path_to_data))\n",
    "\n",
    "    # calculates how many train data per class\n",
    "    data_counter_per_class = np.zeros((len(dirs)))\n",
    "    for i in range(len(dirs)):\n",
    "        path = os.path.join(path_to_data, dirs[i])\n",
    "        files = get_files_from_folder(path)\n",
    "        data_counter_per_class[i] = len(files)\n",
    "    test_counter = np.round(data_counter_per_class * (1 - train_ratio))\n",
    "\n",
    "    # transfers files\n",
    "    for i in range(len(dirs)):\n",
    "        path_to_original = os.path.join(path_to_data, dirs[i])\n",
    "        path_to_save = os.path.join(path_to_test_data, dirs[i])\n",
    "\n",
    "        #creates dir\n",
    "        if not os.path.exists(path_to_save):\n",
    "            os.makedirs(path_to_save)\n",
    "        files = get_files_from_folder(path_to_original)\n",
    "        # moves data\n",
    "        for j in range(int(test_counter[i])):\n",
    "            dst = os.path.join(path_to_save, files[j])\n",
    "            src = os.path.join(path_to_original, files[j])\n",
    "            shutil.move(src, dst)\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "  parser = argparse.ArgumentParser(description=\"Dataset divider\")\n",
    "  parser.add_argument(\"--data_path\", required=True,\n",
    "    help=\"Path to data\")\n",
    "  parser.add_argument(\"--test_data_path_to_save\", required=True,\n",
    "    help=\"Path to test data where to save\")\n",
    "  parser.add_argument(\"--train_ratio\", required=True,\n",
    "    help=\"Train ratio - 0.7 means splitting data in 70 % train and 30 % test\")\n",
    "  return parser.parse_args()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  args = parse_args()\n",
    "  main(args.data_path, args.test_data_path_to_save, float(args.train_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79508ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b093d273",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee6c939",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314cd753",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "73347267",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dims = (28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f2d6cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 2\n",
    "encoder = Sequential([\n",
    "    layers.Input(shape = image_dims),\n",
    "    layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\"),\n",
    "    layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\"),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "    ##Output is two numbers\n",
    "    layers.Dense(latent_dim)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ba021c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 14, 14, 32)        320       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 7, 7, 64)          18496     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 3136)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                50192     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 34        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 69,042\n",
      "Trainable params: 69,042\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9787aed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dims = (28, 28, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "226817d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "quarter = int(image_dims[0]/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b2a2b9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Sequential([\n",
    "    keras.Input(shape=(latent_dim,)),\n",
    "    layers.Dense(quarter * quarter * 64, activation=\"relu\"),\n",
    "    layers.Reshape((quarter, quarter, 64)),\n",
    "    layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\"),\n",
    "    layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\"),\n",
    "    layers.Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "41abe402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_2 (Dense)             (None, 3136)              9408      \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 7, 7, 64)          0         \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTra  (None, 14, 14, 64)       36928     \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2DT  (None, 28, 28, 32)       18464     \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_transpose_2 (Conv2DT  (None, 28, 28, 1)        289       \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 65,089\n",
      "Trainable params: 65,089\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c15de60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_encoder = Sequential([encoder, decoder])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c565d298",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'iloc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [39]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m      2\u001b[0m y \u001b[38;5;241m=\u001b[39m path\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'iloc'"
     ]
    }
   ],
   "source": [
    "X = path.iloc[:, :-1]\n",
    "y = path.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a369469e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [38]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m (x_train, _), (x_test, _) \u001b[38;5;241m=\u001b[39m path\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m##Join test and train sets together\u001b[39;00m\n\u001b[0;32m      3\u001b[0m mnist_digits \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([x_train, x_test], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "(x_train, _), (x_test, _) = path\n",
    "##Join test and train sets together\n",
    "mnist_digits = np.concatenate([x_train, x_test], axis=0)\n",
    "##Normalise down to 0-1\n",
    "mnist_digits = np.expand_dims(path, -1).astype(\"float32\") / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2986a0fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
